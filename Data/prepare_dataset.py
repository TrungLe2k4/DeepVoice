import os
import glob
import random
import librosa
import soundfile as sf
from pydub import AudioSegment, effects
import numpy as np
import pandas as pd
from tqdm import tqdm
from multiprocessing import Pool, cpu_count

# ========== C·∫•u h√¨nh ==========
input_root  = r"D:\DeepVoice\Data\Raw"
output_root = r"D:\DeepVoice\Data\Cleaned"
metadata_csv = r"D:\DeepVoice\Data\metadata_master.csv"
error_log    = r"D:\DeepVoice\Data\convert_audio_error_log.txt"

target_sr        = 16000
target_duration  = 5.0                 # ƒë·ªô d√†i chu·∫©n ho√° cu·ªëi c√πng (s)
target_length    = int(target_sr * target_duration)
labels           = ["real", "fake"]

# Ch·ªâ y√™u c·∫ßu file sau TRIM c√≥ ƒë·ªô d√†i >= 2s
min_duration     = 2.0                 # t·ªëi thi·ªÉu 2 gi√¢y

min_rms          = 0.01
trim_top_db      = 30
allowed_exts     = (".wav", ".mp3", ".flac", ".m4a", ".ogg")

SPLIT = {"train": 0.8, "val": 0.1, "test": 0.1}


# ========== Ti·ªán √≠ch ==========
def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def pick_set():
    r = random.random()
    for k, p in SPLIT.items():
        if r <= p:
            return k
        r -= p
    return "train"


def normalize_audiosegment(seg: AudioSegment) -> AudioSegment:
    seg = seg.set_channels(1)
    seg = effects.normalize(seg)
    return seg


def pad_or_trim(y: np.ndarray, target_len: int) -> np.ndarray:
    """
    Pad ho·∫∑c c·∫Øt t√≠n hi·ªáu audio v·ªÅ ƒë√∫ng ƒë·ªô d√†i target_len m·∫´u.
    Thay th·∫ø cho librosa.util.fix_length ƒë·ªÉ tr√°nh l·ªói version.
    """
    cur_len = len(y)
    if cur_len > target_len:
        return y[:target_len]
    if cur_len < target_len:
        pad_width = target_len - cur_len
        return np.pad(y, (0, pad_width), mode="constant")
    return y


# ========== H√†m x·ª≠ l√Ω t·ª´ng file ==========
def process_one(args):
    ip, label, input_root, output_root = args
    try:
        audio = AudioSegment.from_file(ip)
        audio = audio.set_channels(1)

        # ƒê·ªô d√†i g·ªëc (ch·ªâ d√πng ƒë·ªÉ log / b·ªè c·ª±c k·ª≥ ng·∫Øn)
        dur = audio.duration_seconds
        if dur < 0.5:
            return None, f"{ip} | Qu√° ng·∫Øn g·ªëc ({dur:.2f}s)"

        audio = normalize_audiosegment(audio)

        # L·∫•y samples d·∫°ng float32 [-1, 1]
        samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32768.0

        # L·ªçc theo RMS (√¢m l∆∞·ª£ng)
        rms = float(np.sqrt(np.mean(samples ** 2)))
        if rms < min_rms:
            return None, f"{ip} | √Çm l∆∞·ª£ng th·∫•p (RMS={rms:.4f})"

        y = samples

        # Resample v·ªÅ target_sr
        if audio.frame_rate != target_sr:
            y = librosa.resample(y=y, orig_sr=audio.frame_rate, target_sr=target_sr)

        # Trim silence
        y_trim, _ = librosa.effects.trim(y, top_db=trim_top_db)
        trim_len_sec = len(y_trim) / target_sr

        # ‚ùó Ch·ªâ lo·∫°i file qu√° ng·∫Øn sau trim (< 2s)
        if trim_len_sec < min_duration:
            return None, f"{ip} | Qu√° ng·∫Øn sau trim (<2s: {trim_len_sec:.2f}s)"

        # Chu·∫©n ho√° ƒë·ªô d√†i cu·ªëi c√πng = 5s:
        # - N·∫øu > 5s: C·∫ÆT xu·ªëng 5s
        # - N·∫øu 2‚Äì5s: PAD th√™m im l·∫∑ng cho ƒë·ªß 5s
        y_final = pad_or_trim(y_trim, target_length)

        # Chu·∫©n ho√° bi√™n ƒë·ªô [-1, 1]
        y_final = y_final / (np.max(np.abs(y_final)) + 1e-9)

        # ===== T√çNH ƒê·∫∂C TR∆ØNG ƒê·ªÇ L·ªåC TR√ôNG =====
        # MFCC 13 chi·ªÅu, l·∫•y trung b√¨nh theo th·ªùi gian ‚Üí vector (13,)
        mfcc = librosa.feature.mfcc(y=y_final, sr=target_sr, n_mfcc=13)
        mfcc_mean = np.mean(mfcc, axis=1)
        # L∆∞·ª£ng t·ª≠ ho√° 4 ch·ªØ s·ªë th·∫≠p ph√¢n ƒë·ªÉ ·ªïn ƒë·ªãnh
        feat_key = "|".join(f"{v:.4f}" for v in mfcc_mean)

        # ===== L∆ØU FILE WAV =====
        base_name = os.path.splitext(os.path.basename(ip))[0]
        out_label_dir = os.path.join(output_root, label)
        ensure_dir(out_label_dir)
        op = os.path.join(out_label_dir, f"{base_name}.wav")
        sf.write(op, y_final, target_sr)

        return {
            "file_path": f"{label}/{base_name}.wav",
            "label": label,
            "duration": round(len(y_final) / target_sr, 3),
            "rms": round(float(np.sqrt(np.mean(y_final ** 2))), 5),
            "set": pick_set(),
            "feat_key": feat_key,
        }, None

    except Exception as e:
        return None, f"{ip} | {str(e)}"


# ========== Main ==========
def main():
    random.seed(42)
    ensure_dir(output_root)

    # Thu th·∫≠p danh s√°ch file ƒë·∫ßu v√†o
    all_files = []
    for label in labels:
        in_label_dir = os.path.join(input_root, label)
        if not os.path.isdir(in_label_dir):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {in_label_dir}")
            continue
        for ext in allowed_exts:
            all_files.extend(
                [(fp, label, input_root, output_root)
                 for fp in glob.glob(os.path.join(in_label_dir, f"*{ext}"))]
            )

    print(f"üîç T·ªïng s·ªë file c·∫ßn x·ª≠ l√Ω: {len(all_files)}")
    if not all_files:
        return

    n_jobs = min(cpu_count(), 8)
    print(f"‚öôÔ∏è ƒêang s·ª≠ d·ª•ng {n_jobs} CPU l√µi song song...")

    rows, errors = [], []
    with Pool(processes=n_jobs) as pool:
        for result, err in tqdm(pool.imap_unordered(process_one, all_files), total=len(all_files)):
            if result:
                rows.append(result)
            if err:
                errors.append(err)

    # ===== Ghi metadata & lo·∫°i b·ªè file tr√πng ƒë·∫∑c tr∆∞ng =====
    if rows:
        df = pd.DataFrame(rows)

        # N·∫øu c√≥ c·ªôt feat_key ‚Üí l·ªçc tr√πng
        if "feat_key" in df.columns:
            # Gi·ªØ l·∫°i 1 b·∫£n ghi ƒë·∫ßu ti√™n cho m·ªói feat_key
            df_unique = df.drop_duplicates(subset=["feat_key"], keep="first").copy()

            # X√°c ƒë·ªãnh c√°c file b·ªã lo·∫°i (tr√πng ƒë·∫∑c tr∆∞ng)
            dup_mask = ~df["file_path"].isin(df_unique["file_path"])
            dup_files = df.loc[dup_mask, "file_path"].tolist()

            removed_count = 0
            for relpath in dup_files:
                full_path = os.path.join(output_root, relpath)
                if os.path.isfile(full_path):
                    try:
                        os.remove(full_path)
                        removed_count += 1
                    except Exception as e:
                        print(f"‚ö†Ô∏è Kh√¥ng xo√° ƒë∆∞·ª£c file tr√πng {full_path}: {e}")

            if removed_count:
                print(f"üßπ ƒê√£ lo·∫°i b·ªè {removed_count} file c√≥ ƒë·∫∑c tr∆∞ng tr√πng nhau.")

            # S·ª≠ d·ª•ng df_unique cho metadata cu·ªëi
            df = df_unique

        # Ghi metadata CSV
        df.to_csv(metadata_csv, index=False, encoding="utf-8")
        print(f"‚úÖ ƒê√£ l∆∞u metadata: {metadata_csv} ({len(df)} file)")
        print(df.groupby(["label", "set"]).size())

    # Ghi log l·ªói
    if errors:
        with open(error_log, "w", encoding="utf-8") as f:
            for msg in errors:
                f.write(msg + "\n")
        print(f"‚ö†Ô∏è ƒê√£ ghi log l·ªói: {error_log} ({len(errors)} l·ªói)")

    print("üéâ Ho√†n t·∫•t chu·∫©n ho√° dataset .")


if __name__ == "__main__":
    main()
